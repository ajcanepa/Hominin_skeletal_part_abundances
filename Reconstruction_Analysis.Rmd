---
title: "Reconstrucción de Egeland et al., 2018: 'Hominin skeletal part abundances...'"
author: 'Antonio Canepa-Oneto'
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes  
    toc_float: yes
    toc_depth: 6
    keep_md: yes
---

## Introducción
Se hace una mini-revisión de lo reportado en *Hominin skeletal part abundances and claims of deliberate disposal of corpses in the Middle Pleistocene* [https://doi.org/10.1073/pnas.1718678115](https://doi.org/10.1073/pnas.1718678115)

Los procesos funerarios humanos tiene una antiguedad determinada (desde cuándo somos conscientes de nuestra muerte) que está bajo escrutinio y constante debate. Gracias a un estudio de diversos yacimientos se intenta esclarecer ese posible supuesto de primer origen de enterramientos funerarios para los yacimientos de Atapuerca (**SH**) y Sudáfrica (**DC**).

Cuando se encuentra y analiza un yacimiento de homínidos (u homininos para ser más precisos) es posible que no todos los huesos estén en perfecto estado y más notable aún es posible que no todos los huesos estén presentes. La ausencia de determinados huesos puede deberse a procesos aleatorios (deterioro, rompimiento, etc), semi-aleatorios (consumo de carroña de partes específicas) o altamente selectivos (ritos funerarios).

Afortunamente existe una cantidad interesante de yacimientos con restos humanos y de primates ("cercanos") que pueden dar pistas de si los yacimientos **SH** y **DC** pueden (o no) considerarse procesos rituales (muy específicos) o simplemente procesos aleatorios y/o semi-aleatorios.

### Objetivos
Los objetivos del artículo son tratar de comparar los yacimientos de **SH** y **DC** con yacimientos conocidos para determinar si la estructura (como composición unitaria, no como forma) de huesos disponibles es ás característico de un rito funerario o de un proceso de pérdida aleatorio (descomposición) o semi-aleatorio (carroña).

#### Diseño de Análisis
La idea general es que ellos comparan los huesos encontrados ("*Hominin skeletal part representation* - __HSPR__") con otros 14 yacimientos entre los cuales existen algunos de humanos en estado "completo" en el que no le falta ninguna pieza.

Para eso ellos:

1.-   Usando un Random Forest (RF) determinan cuáles son las piezas de huesos con más variabilidad (Gini idex y MDA), para utilizarlas luego en la creación/clasificación de grupos.

2.-   Usando un primer k-means logran crear dos grupos de "HSPR", en los que los huesos humanos "completos" y otros yacimientos en buen estado quedan juntos en un cluster y el segundo cluster atribuyen a carroña, canibalismo, etc. mostrando un proceso de acumulación natural (__no ritual__).

3.-   Repiten el k-means con más grupos y encuentran un gradiente de conservación de 4 grupos desde los más completos hasta los netamente depredados/erosionados.

4.-   Usando herramientas supervisadas intentan clasificar los yacimientos (todos) con especial interés en **SH** y **DC** en alguno de los grupos ya reconocidos por los k-means anteriores.

5.-   Usando herramientas no-supervisadas intentan agrupar los yacimientos (todos) con especial interés en **SH** y **DC** para ver si coinciden con las características/grupos encontradas en los k-means anteriores.

### Tratamiento estadístico de datos de partes esqueléticas de primates
Todos los análisis descritos a continuación se ejecutan dentro del entorno estadístico R

Los algoritmos implementados, el paquete utilizado y la URL de esos paquetes se enumeran en la siguiente tabla:


```{r echo=FALSE}
packages <- tibble(
  Algorithm = c("Neural Network",
                "Neural Network",
                "SVM", 
                "SVM", 
                "Decision tree with C5.0", 
                "Decision tree with C5.0",
                "KNN",
                "KNN",
                "Random Forest",
                "Random Forest"
                ),
  Package_used = c("neuralnet", 
                    "caret", 
                    "e1071", 
                    "caret", 
                    "C50",
                    "caret",
                    "class",
                    "knn/fknn",
                    "randomForest",
                    "caret"
                    ),
  Packages_Link = c('<a href="https://cran.r-project.org/web/packages/neuralnet/">https://cran.r-project.org/web/packages/neuralnet/</a>',
                    '<a href="https://cran.r-project.org/web/packages/caret/index.html">https://cran.r-project.org/web/packages/caret/index.html</a>',
                    '<a href="https://cran.r-project.org/web/packages/e1071/index.html">https://cran.r-project.org/web/packages/e1071/index.html</a>',
                    '<a href="https://cran.r-project.org/web/packages/caret/index.html">https://cran.r-project.org/web/packages/caret/index.html</a>',
                    '<a href="https://cran.r-project.org/web/packages/C50/index.html">https://cran.r-project.org/web/packages/C50/index.html</a>',
                    '<a href="https://cran.r-project.org/web/packages/caret/index.html">https://cran.r-project.org/web/packages/caret/index.html</a>',
                    '<a href="https://cran.r-project.org/web/packages/class/index.html">https://cran.r-project.org/web/packages/class/index.html</a>',
                    '<a href="https://cran.r-project.org/web/packages/FastKNN/index.html">https://cran.r-project.org/web/packages/FastKNN/index.html</a>',
                    '<a href="https://cran.r-project.org/web/packages/randomForest/index.html">https://cran.r-project.org/web/packages/randomForest/index.html</a>',
                    '<a href="https://cran.r-project.org/web/packages/caret/index.html">https://cran.r-project.org/web/packages/caret/index.html</a>' )
)

#packages
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
datatable(packages, escape = FALSE,  width = '100%', options = list(scrollX = TRUE))  # This does show the links
```


```{r, echo=FALSE, eval=FALSE}
# # using markdown
# packages %>%
#   mutate(
#     link = glue::glue("[website]({Packages_Link})"),
#     link = map(Packages_Link, gt::md)) %>%
#   select(!Packages_Link) %>% 
#   gt() %>% 
#   tab_header(
#     title = md("List of algorithms used in Egeland et al., 2018"),
#     subtitle = "R Packages and their URL are given"
#   ) %>% 
#   cols_label(
#     Algorithm = "Algorithm",
#     Package_used = "Package used",
#     link = "Package URL") %>%  
#   tab_footnote(data = .,
#                footnote = "No 'knn' package was found. Similar algorithm in FastKNN - fknn",
#                # locations = cells_body(columns = Package_used, rows = everything()
#                locations = cells_body(columns = Package_used, rows = Package_used == 'knn/fknn')
#   )
```

### Carga y pre-procesamiento de datos
El conjunto de datos se entrega de manera desagregada, dificultando los análisis por lo que he procedido a recopilarlo todo en un único documento. El documento original entregado como "material suplementario" se puede encontrar [aquí](https://www.pnas.org/doi/suppl/10.1073/pnas.1718678115/suppl_file/pnas.1718678115.sd01.xlsx).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#library(gt)
library(tidyverse)
# Install the DT package
#install.packages("DT")
# Load the DT package
library(DT)
library(knitr)
```

El conjunto de datos recopilado se muestra a continuación. La tabla se puede descargar de la siguiente botonera y es dinámica.


```{r, message=FALSE, warning=FALSE, echo=FALSE}
Summary_Dataset <- read_csv("INPUT/DATA/Summary_Dataset.csv")
```

```{r, echo=FALSE, eval=FALSE}
# Código para crear las categorías del artículo.
Summary_Dataset <-
  Summary_Dataset %>% 
  mutate(Assem = case_when(grepl("Hummingbird_Pueblo_Pueblo", Study) ~ ""))
```


```{r, message=FALSE, warning=FALSE, echo=FALSE, eval=FALSE}
#Tablas pijas con GT que funcionan muy mal
# Summary_Dataset %>% 
#   slice_head(n = 15) %>% 
#   gt()
# 
# fmt_markdown(
#   gt(Summary_Dataset),
#   columns = everything(),
#   rows = everything(),
#   md_engine = c("markdown", "commonmark")
# )
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# put CSV, XLS, and PDF in a collection
datatable(
  Summary_Dataset, width = '100%', 
  #options = list(scrollX = TRUE),
  extensions = 'Buttons', 
  options = list(
   scrollX = TRUE,
    dom = 'Bfrtip',
    buttons = 
      list('copy', 'print', list(
        extend = 'collection',
        buttons = c('csv', 'excel', 'pdf'),
        text = 'Download'
      ))
    
  )
)
```

El conjunto de dato no posee todas las columnas (atributos) utilizadas en el análisis, por lo que tendremos que crear los conjuntos de datos necesarios para replicar los análisis realizados.

*    Suposición: Asumimos que la clase a predecir es una categoría con los niveles `Primary hominin interment`, `Possible primary hominin interment`,  `Hominin cannibalism/secondary interment`, `Nonanthropogenic hominin accumulation`, `Unscavenged human corpses`, `Scavenged human corpses`, `Leopard refuse`, `Natural Baboon accumulation` and `Possible hominin deliberate disposal`.

A continuación crearemos una columna nueva con esta nueva clase a predcir (nuevo atributo `Type`):

```{r message=FALSE, warning=FALSE}
Summary_Dataset_Multivar <-
  Summary_Dataset %>%
  mutate(Type = case_when(grepl(paste(c("Pueblo", "Dolni_Vestonice"), collapse='|'), Ref) ~ "Primary hominin interment",
                          grepl(paste(c("Skhul", "Qafzeh", "Regourdou", "La_Chapelle", "Tabun_Layer", "Shanidar_Layer", "Kebara", "El_Miron"), collapse='|'), Ref) ~ "Possible Primary hominin interment",
                          grepl(paste(c("Krapina", "Fontbregoua", "Gran_Dolina", "El_Mirador", "Goughs_Cave", "5MT", "La_Tolita", "Crow_Creek"), collapse='|'), Ref) ~ "Hominin canibalism/ secondary interment",
                          grepl(paste(c("AL_333", "Liang_Bua", "Dmanisi_Layer", "Malapa"), collapse='|'), Ref) ~ "Nonantrhopogenic hominin accumulation",
                          grepl(paste(c("Unscavenged_human_corpses_WA"), collapse='|'), Ref) ~ "Unscavenged human corpses",
                          grepl(c("Scavenged_human"), Ref) ~ "Scavenged human corpses",
                          grepl(paste(c("Mapungubwe", "Leopard_refuse"), collapse='|'), Ref) ~ "Leopard refuse",
                          grepl(c("Misgrot_Cave"), Ref) ~ "Natural Baboon accumulation",
                          grepl(paste(c("Sima_de_los_Huesos", "Dinaledi"), collapse='|'), Ref) ~ "Possible hominin deliberate disposal",
                          )) %>% 
  relocate(Type, .before = Cranium)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# put CSV, XLS, and PDF in a collection
DT::datatable(
  Summary_Dataset_Multivar, 
  width = '100%', 
  #options = list(scrollX = TRUE),
  extensions = 'Buttons', 
  options = list(
   scrollX = TRUE,
   pageLength = 6,
    dom = 'Bfrtip',
    buttons = 
      list('copy', 'print', list(
        extend = 'collection',
        buttons = c('csv', 'excel', 'pdf'),
        text = 'Download'
      ))
  )
)
```

Guardaremos ahora este conjunto **completo** de datos como `Summary_Dataset_Multivar.csv`.

```{r echo=TRUE, message=FALSE, warning=FALSE}
write_csv(x = Summary_Dataset_Multivar, file = "OUTPUT/Summary_Dataset_Multivar.csv", col_names = TRUE)
```

*    Primate Skeletal Element Data

El total de ensambles de primates fue de `r length(levels(factor(Summary_Dataset$Ref)))`. De éstos, se seleccionaron 14 además de los de la sima de los huesos (**SH**) y Dinaledi Chamber (**DC**). Ver los supplemental information para dilucidar los conjuntos de datos "*seleccionados*"


```{r message=FALSE, warning=FALSE}
filter(Ref %in% c("Pottery_Mound_Pueblo_IV", "Kuaua_Pueblo_Pueblo_IV", "Skhul_Layer_B", "Krapina", "Fontbregoua_H1", "Fontbregoua_H3", "Gran_Dolina_TD6", "El_Mirador_MIR4A", "AL_333", "Unscavenged_human_corpses_WA", "Scavenged_human_corpses_WA", "Mapungubwe_leopard_kills",  "Leopard_refuse", "Misgrot_Cave", "Sima_de_los_Huesos", "Dinaledi")) %>%
```







### Análisis somero del material suplementario

Aquí se destacan algunos puntos clave del documento mismo.

El objetivo del análisis exploratorio es doble: (i) identificar el número óptimo de grupos representados por los ensamblajes de homínidos y (ii) determinar la pertenencia de cada grupo identificado. Muchos algoritmos de agrupación tienden a funcionar mal cuando, como es el caso aquí, el número de variables (elementos esqueléticos = 23) excede sustancialmente el tamaño de la muestra (ensamblajes de homínidos = 16). Para abordar esta discrepancia, utilizamos un análisis de RF en los 16 ensamblajes, incluidos SH y DC, para identificar un subconjunto de elementos esqueléticos que es más pequeño que el tamaño de la muestra y explica la mayor cantidad de variación (120). Para identificar el número óptimo de grupos representados por los 16 ensamblajes, los elementos esqueléticos con un valor de MDA > 5 después de la generación de 500 árboles se ingresan en la biblioteca R "NbClust", que ejecuta y combina 30 algoritmos de agrupamiento diferentes. Luego, un análisis de k-medias clasifica cada uno de los conjuntos comparativos en uno de los grupos reconocidos por las funciones NbClust. La fuerza de la asignación de grupos se evalúa con la función gráfica "clusplot", que proporciona elipses y gráficos de silueta con un 95 % de confianza, que estiman el valor s(i) de cada conjunto comparativo. Una comparación de distancias dentro y entre grupos da como resultado valores de s(i) que van desde 1 (clasificación fuerte dentro de un grupo) hasta 0 (clasificación parsimoniosa pero débil dentro de un grupo). Esta clasificación preliminar establece un marco para la aplicación de una variedad de métodos de aprendizaje automático que pueden identificar los ensamblajes comparativos que mejor coinciden con las concentraciones de homínidos de SH y DC.

Antes de la construcción del modelo, todos los datos de las partes del esqueleto se someten a una transformación de centro y escala

Para elegir el mejor modelo para el análisis, utilizamos el remuestreo de validación cruzada con exclusión de grupo de Monte-Carlo. Esto crea múltiples divisiones de conjuntos de entrenamiento/conjuntos de prueba y es más sólido con muestras pequeñas que los métodos de validación cruzada de bootstrapping, bagging y k-fold.

Los modelos producidos por cada método de aprendizaje automático después de 30 iteraciones se evalúan con κ de Cohen.

Finalmente, realizamos una CA no supervisada con puntajes de carga de PCA.

## Pasos a seguir y algunas ideas
Aseguran que los huesos de *SH* presentan mucha perturbación (tipo carroña) y por ende no podrían ser clasificados como rito mortuorio. Sin embargo no muestran un análisis detallado de los rastros de "huellas" de carroñeo que presentan ni los datos que (entiendo) posee el grupo de Nohemí.

Quizás a nivel de Algoritmo (como tal) no se puede indagar mucho (porque probablemente muestren algo similar), pero sí a nivel de incluir datos nuevos que no estén incluidos y ya sea repitiendo la aproximación de ellos o agregando alguna nueva, los resultados sean diferentes apoyando (esta vez) la idea de que los restos en **SH** están allí por un acto "ritual de muerte" y no por simple acumulación transporte.


## Repetición de los análisis

### Exploratory Analysis

#### Random Forest (RF)
Se realizó un RF a los 16 assemblages para evaluar que atributos (partes del cuerpo) poseían una mayor variabilidad, a través del MDA (*Mean Decrease Accuracy*) y el MDG (*Mean Decrease Gini*).

Encontraron que siete (7) elementos esqueléticos (o grupos de elementos) fueron aquellos que tuvieron un MDA > 5. 






#### K-means análisis
