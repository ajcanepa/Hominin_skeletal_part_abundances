---
title: "MAU"
format: html
---

### Import packages

```{r message=FALSE, warning=FALSE}
set.seed(42)
source("../INPUT/FUNCTIONS/preprocess.R")
library(dplyr)
library(rsample)
library(recipes)
library(randomForest)
library(corrplot)
library(tibble)
library(purrr)
library(reshape2)
library(ggplot2)
library(plotly)
library(scatterplot3d)
library(factoextra)
library(cluster)
library(tidyverse)
```

# %MAU - Used

```{r}
data <- read.csv('../Input/DATA/Summary_Dataset_Used_Percentage_MAU.csv')
data_multi <- read.csv('../Input/DATA/Summary_Dataset_Multivar_Percentage_MAU.csv')
```

```{r}
data = preprocess(data)
data_multi = preprocess(data_multi)
glimpse(data)
```

```{r}
rownames(data)
```

```{r}
# Define the columns for which you want to count the labels
columns_to_count <- c("Type", "AccumulationType", "Individuals", "Cluster_Pnas")

# Function to count labels in each specified column
count_labels <- function(data, columns) {
  sapply(data[columns], function(column) table(column))
}

# Use the function to count labels
label_counts <- count_labels(data, columns_to_count)

# Print the counts
label_counts

```

### Take SH out of the training set (to avoid data leakage)

```{r}
split <- split_train_SH(data)

data <- split$train_data
sh <- split$sh_data
```

### Center and scale numeric data

Normalize numeric data to have a standard deviation of one and a mean of zero.

```{r,warning=FALSE}
# Store original row names of the datasets
data_row_names <- rownames(data)
sh_row_names <- rownames(sh)

# Step 1: Create a recipe
recipe <- recipe(~ ., data = data) %>%
  step_normalize(all_predictors(), -all_of(c("Type", "AccumulationType", "Individuals", "Cluster_Pnas")))

# Step 2: Fit the recipe to the training set
fitted_recipe <- prep(recipe, training = data)

# Step 3: Apply the transformation to both training and test sets
data_transformed <- bake(fitted_recipe, new_data = data)
sh_transformed <- bake(fitted_recipe, new_data = sh)

# Reattach the row names
rownames(data_transformed) <- data_row_names
rownames(sh_transformed) <- sh_row_names
```

### Feature selection

**In Pnas RF with labels has been used (!data leakage).**

```{r}
# Ensure reproducibility
set.seed(42)

# Prepare the label and features
label <- data$Type
data_rf_sup <- data %>% select(-c("Type", "AccumulationType", "Individuals", "Cluster_Pnas"))

# Convert 'Type' to a factor
label <- as.factor(data$Type)

# Train the RandomForest model
rf_model_sup <- randomForest(x = data_rf_sup, y = label, importance = TRUE)

# Get the feature importance
feature_importance_ <- importance(rf_model_sup)

# Print the feature importance
feature_importance_[,c(10:11)][]

```

```{r}
# Selecting MeanDecreaseAccuracy 
mean_decrease_accuracy <- feature_importance_[, "MeanDecreaseAccuracy"]

# Filter features where MeanDecreaseAccuracy is greater than 5
important_features_ <- feature_importance_[mean_decrease_accuracy > 3.4, ]

# Print the important features
important_features_[,c(10:11)]
```

```{r}
# Save top 7 features
columns_rf_sup<- rownames(as.data.frame(important_features_[,c(10:11)]))
columns_rf_sup
```

**Random Forrest feature importance with self-label (without data leakage)**

```{r}
set.seed(42)
data_rf <- data |> select(-c("Type", "AccumulationType", "Individuals", "Cluster_Pnas"))

# Create artificial labels where each row is its own class
artificial_labels <- seq_len(nrow(data_rf))

# Combine the data with the artificial labels
data_with_labels <- cbind(data_rf, artificial_labels)

# Train Random Forest model
rf_model <- randomForest(artificial_labels ~ ., data = data_with_labels, importance = TRUE)

# Extract feature importance based on Mean Decrease in Gini
feature_importance <- importance(rf_model) # type = 1 for %IncMSE 

feature_importance
```

We are going to use `names(featrue_importance[feature_importance > 2])`

```{r}
# Selecting MeanDecreaseAccuracy 
mean_decrease_accuracy <- feature_importance[,"%IncMSE"]

# Filter features where MeanDecreaseAccuracy is greater than 5
important_features_ <- feature_importance_[mean_decrease_accuracy > 2, ]

# Print the important features
columns_rf <- rownames(important_features_[,c(10:11)])
columns_rf
```

**Unsupervised Feature Selection with Random Forests (UFSRF)**

```{r}
#Todo
```

**SHAP**

```{r}
#Todo

```

**Feature correlation heatmap**

```{r}
data_plot_cor = abbreviated_columns(data)
```

```{r}
# Correlation matrix
cor_matrix <- cor(data_plot_cor[sapply(data_plot_cor, is.numeric)], use = "complete.obs")

melted_cor_matrix <- melt(cor_matrix)
ggplot(melted_cor_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + # Rotate x-axis labels
  #theme_minimal() +
  labs(title = "Correlation Matrix", x = "", y = "")
```

```{r}
corrplot(cor_matrix, #correlation matrix
         method = "color", #show coloured blocks
         is.corr = TRUE, #input matrix is a correlation matrix
         type = "upper", #show only upper diagonal of correlogram
         order = "AOE", #Hierarchical clustering order
         hclust.method = "average", #using the average linkage distance measure
         addCoef.col = "black", #show numerical value of coefficient
         tl.col = "black", #text color of data label
         tl.cex = 0.5, #text size of data label
         rect.lwd = 3,#text size of data label
         tl.srt = 90, #text rotation of data label
         cl.cex = 1, #text size of color-legend label
         number.cex = 0.5, #text size of correlation coefficient
         diag = FALSE, #hide coefficient on the principal diagonal
)
```

```{r}
corrplot(cor_matrix, #correlation matrix
         method = "color", #show coloured blocks
         is.corr = TRUE, #input matrix is a correlation matrix
         type = "upper", #show only upper diagonal of correlogram
         order = "hclust", #Hierarchical clustering order
         hclust.method = "average", #using the average linkage distance measure
         addCoef.col = "black", #show numerical value of coefficient
         tl.col = "black", #text color of data label
         tl.cex = 0.5, #text size of data label
         rect.lwd = 3,#text size of data label
         tl.srt = 90, #text rotation of data label
         cl.cex = 1, #text size of color-legend label
         number.cex = 0.5, #text size of correlation coefficient
         diag = FALSE, #hide coefficient on the principal diagonal
)
```

**Based on feature correlation and clustering, merge bone subcategories into its categories:**

-   *"Vertebra"= {"Cervical", "Thoracic", "Lumbar", "Sacrum","Rib"}*

-   *"Arm" = {"Humerus", "Radius", "Ulna"}*

-   *"Leg" = {"Femur", "Tibia", "Fibula" }*

-   *"Extremities": {"Carpal", "Tarsal*", "Hand..metacarpals...manual.phalanges.", "Foot..metatarsals...pedal.phal

```{r}
## Define data_cor data set
data_cor = merge_bone_cat_by_mean(data)
```

**Define list of data sets to work with:**

```{r}
list_of_dfs <- list(data, data[columns_rf_sup] , data[columns_rf], data_cor)
```

### Dimension Reduction

**PCA**

```{r}
# PCA
data_pca <- data |> select(-all_of(columns_to_count))
pca_result <- prcomp(data_pca, center = TRUE, scale. = TRUE)
X_pca <- pca_result$x[,1:6]
X_pca
```

```{r}
## Plot 3D pca
df_X_pca = as.data.frame(X_pca)
plot_ly(df_X_pca,x=~PC1,y=~PC2,z=~PC3, color = ~label, type = 'scatter3d', mode = 'markers')


```

```{r}

# Perform PCA
pca_result <- prcomp(data_pca, scale. = TRUE)

# Calculate cumulative variance explained by each component
var_explained <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))

# Optional: Create a scree plot to visualize the variance explained
ggplot(as.data.frame(var_explained), aes(x = seq_along(var_explained), y = var_explained)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") +
  xlab("Number of Principal Components") +
  ylab("Cumulative Variance Explained") +
  ggtitle("Scree Plot Showing Cumulative Variance Explained by PCA Components")

```

**LDA**

```{r, warning=FALSE}
#library(MASS)

# LDA
#cols_lda <- setdiff(names(data), c("Type","Individuals", "Cluster_Pnas"))
#data_lda <- data[cols_lda]
#lda_result <- lda(AccumulationType ~ ., data = data_lda)
#X_lda <- as.data.frame(predict(lda_result, newdata = data_lda)$x)

# Create a 3D scatter 
#plot_ly(X_lda, x = ~LD1, y = ~LD2, z = ~LD3, color = ~data$AccumulationType, type = 'scatter3d', mode = 'markers')
```

```{r}
# LDA
#cols_lda <- setdiff(names(data), c("AccumulationType","Individuals", "Cluster_Pnas"))
#data_lda <- data[cols_lda]
#lda_result <- lda(Type ~ ., data = data_lda)
#X_lda <- as.data.frame(predict(lda_result, newdata = data_lda)$x)

# Create a 3D scatter 
#plot_ly(X_lda, x = ~LD1, y = ~LD2, z = ~LD3, color = ~data$Type, type = 'scatter3d', mode = 'markers')
```

**amend Dataframe list with X_pca\[,c(1:6)\] and X_lda**

```{R}
list_of_dfs <- list(data, data[, columns_rf_sup], data[, columns_rf], data_cor, X_pca[, 1:6]
                    #,X_lda
                    )
```

### Best K

**Ellbow method**

```{r}
library(cluster)
library(broom)
set.seed(42)
cols_km <- setdiff(names(data), c("Type","AccumulationType","Individuals", "Cluster_Pnas"))
data_km <- data[cols_km]
# Create 10 models with 1 to 10 clusters
kclusts <- tibble(k = 1:7) %>% 
  mutate(
    model = map(k, ~ kmeans(x = data_km, centers = .x, nstart = 20)),
    glanced = map(model, glance)
  ) %>% 
  unnest(cols = c(glanced))

# View results
kclusts%>% 
  ggplot(mapping = aes(x = k, y = tot.withinss)) +
  geom_line(size = 1.2, alpha = 0.5, color = "blue") +
  geom_point(size = 2, color = "blue")
```

```{r}
library(cluster)
library(factoextra)

#create plot of number of clusters vs total within sum of squares
fviz_nbclust(data_km, kmeans, method = "wss")
```

**Silhouette method**

```{r}
fviz_nbclust(data_km, FUN=kmeans, method = "silhouette")
```

```{r}
gap_stat <- clusGap(data_km, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

### Modeling

```{r}
# Load necessary libraries
library(factoextra)
library(cluster)

#Perform K-means clustering
set.seed(42)  
kmeans_result <- kmeans(data_km, centers = 3, nstart = 25)  

# The cluster assignments are stored in kmeans_result$cluster
sub_grp <- kmeans_result$cluster

# Visualize the clusters
fviz_cluster(list(data = data_km, cluster = sub_grp))

```

### Eval

```{r}
# Perform K-means clustering on the training set
set.seed(42) 
kmeans_result <- kmeans(data_km, centers = 3, nstart = 25)

# Assign test set points to nearest clusters
get_nearest_cluster <- function(point, centers) {
  distances <- apply(centers, 1, function(center) dist(rbind(center, point)))
  return(which.min(distances))
}

# Apply the function to each row in the test set
cluster_assignments_test <- apply(sh, 1, get_nearest_cluster, centers = kmeans_result$centers)

sh_km = sh[cols_km] 
# Combine the training and test sets for visualization
combined_df <- rbind(data_km, sh_km)
combined_clusters <- c(kmeans_result$cluster, cluster_assignments_test)

# Visualize with fviz_cluster
fviz_cluster(list(data = combined_df, cluster = combined_clusters))
```

```{r}
# Load necessary libraries
#library(factoextra)
#library(cluster)

# List of dataframes for evaluation
#list_of_dfs <- list(data_km, data[, columns_rf_sup], data[, columns_rf]
                    #,data_cor
#                    ,X_pca[, 1:6])

# Number of clusters to evaluate
#k_values <- c(3, 4, 5)

# List to store results
#results <- list()

# Function to perform K-means clustering and evaluation
#perform_clustering <- function(data, k, method) {
#  set.seed(42)
#  kmeans_result <- kmeans(data, centers = k, nstart = 25, algorithm = method)
#  return(list(cluster = kmeans_result$cluster, centers = kmeans_result$centers))
#}

# Loop through each dataframe
#for (i in seq_along(list_of_dfs)) {
#  df <- list_of_dfs[[i]]
  
  # Loop through each k value
#  for (k in k_values) {
    # Try different initialization algorithms if available
#    for (method in c("Hartigan-Wong", "Lloyd", "MacQueen", "kmeans++")) {
#      clustering_result <- try(perform_clustering(df, k, method), silent = TRUE)
#      if (!inherits(clustering_result, "try-error")) {
        # Store results
#        results[[paste("DataFrame", i, "K", k, "Method", method)]] <- clustering_result
#      }
#    }
#  }
#}

# Visualize the results for one of the combinations (example)
#fviz_cluster(list(data = list_of_dfs[[1]], cluster = results$'DataFrame 1 K 3 Method kmeans++'$cluster))
#results$`DataFrame 2 K 3 Method Hartigan-Wong`$cluster
```

r\)

```{r}

# Load necessary libraries
library(factoextra)
library(cluster)
library(dbscan)


# List of dataframes for evaluation
list_of_dfs <- list(data_km,data[, columns_rf_sup]
                    , data[, columns_rf]
                    #, data_cor 
                    ,X_pca[, 1:6]
                    )

# Number of clusters to evaluate for K-means
k_values <- c(3, 4, 5)

# Hyperparameters for DBSCAN (example ranges)
eps_values <- seq(0.1, 1, 0.2)
minPts_values <- c(5, 10, 15)

# Parameters for hierarchical clustering
hclust_methods <- c("ward.D", "single", "complete", "average")

# List to store results and best models
results <- list()
best_models <- list(kmeans = list(score = -Inf, model = NULL),
                    dbscan = list(score = -Inf, model = NULL),
                    hclust = list(score = -Inf, model = NULL))

# Function to perform K-means clustering
perform_kmeans <- function(data, k, method) {
  set.seed(42)
  kmeans_result <- kmeans(data, centers = k, nstart = 25, algorithm = method)
  silhouette_score <- mean(silhouette(kmeans_result$cluster, dist(data))[, 3])
  return(list(cluster = kmeans_result$cluster, score = silhouette_score))
}

# Function to perform DBSCAN clustering
perform_dbscan <- function(data, eps, minPts) {
  # Perform DBSCAN clustering
  dbscan_result <- dbscan::dbscan(data, eps = eps, minPts = minPts)

  # Exclude noise points (cluster = 0) from silhouette score calculation
  valid_clusters <- dbscan_result$cluster[dbscan_result$cluster != 0]
  valid_data <- data[dbscan_result$cluster != 0, ]

  # Check if there are valid clusters to avoid errors in silhouette calculation
  if (length(unique(valid_clusters)) > 1) {
    silhouette_score <- mean(silhouette(valid_clusters, dist(valid_data))[, 3])
  } else {
    # Assign a default low score if there's only one cluster or none
    silhouette_score <- -1
  }

  return(list(cluster = dbscan_result$cluster, score = silhouette_score))
}


# Function to perform hierarchical clustering
perform_hclust <- function(data, method, k) {
  hc <- hclust(dist(data), method = method)
  clusters <- cutree(hc, k = k)
  silhouette_score <- mean(silhouette(clusters, dist(data))[, 3])
  return(list(cluster = clusters, score = silhouette_score))
}

# Loop through each dataframe
for (i in seq_along(list_of_dfs)) {
  df <- list_of_dfs[[i]]
  
  # K-means
  for (k in k_values) {
    for (method in c("Hartigan-Wong", "Lloyd", "Forgy",
                     "MacQueen")) {
      result <- perform_kmeans(df, k, method)
      if (result$score > best_models$kmeans$score) {
        best_models$kmeans <- list(score = result$score, model = result$cluster, dataframe = i, k = k, method = method)
      }
    }
  }
  
  # DBSCAN
  for (eps in eps_values) {
    for (minPts in minPts_values) {
      result <- perform_dbscan(df, eps, minPts)
      if (result$score > best_models$dbscan$score) {
        best_models$dbscan <- list(score = result$score, model = result$cluster, dataframe = i, eps = eps, minPts = minPts)
      }
    }
  }
  
  # Hierarchical Clustering
  for (method in hclust_methods) {
    for (k in k_values) {
      result <- perform_hclust(df, method, k)
      if (result$score > best_models$hclust$score) {
        best_models$hclust <- list(score = result$score, model = result$cluster, dataframe = i, k = k, method = method)
      }
    }
  }
}

# Best models
best_models

```

```{r}
list_of_dfs <- list(data_km, data[, columns_rf_sup], data[, columns_rf], data_cor, X_pca[, 1:6]
                    #,X_lda
                    )
data[cols_km]
```

# Multivar.

```{r}
#data <- read.csv('../Input/DATA/Summary_Dataset_Multivar_Percentage_MAU.csv')
```

```{r}
data = data_multi  #preprocess(data)
glimpse(data)
```

```{r}
rownames(data)
```

```{r}
# Define the columns for which you want to count the labels
columns_to_count <- c("Type", "AccumulationType", "Individuals")

# Function to count labels in each specified column
count_labels <- function(data, columns) {
  sapply(data[columns], function(column) table(column))
}

# Use the function to count labels
label_counts <- count_labels(data, columns_to_count)

# Print the counts
label_counts

```

### Take SH out of the training set (to avoid data leakage)

```{r}
split <- split_train_SH(data)

data <- split$train_data
sh <- split$sh_data
```

### Center and scale numeric data

Normalize numeric data to have a standard deviation of one and a mean of zero.

```{r,warning=FALSE}
# Store original row names of the datasets
data_row_names <- rownames(data)
sh_row_names <- rownames(sh)

# Step 1: Create a recipe
recipe <- recipe(~ ., data = data) %>%
  step_normalize(all_predictors(), -all_of(c("Type", "AccumulationType", "Individuals")))

# Step 2: Fit the recipe to the training set
fitted_recipe <- prep(recipe, training = data)

# Step 3: Apply the transformation to both training and test sets
data_transformed <- bake(fitted_recipe, new_data = data)
sh_transformed <- bake(fitted_recipe, new_data = sh)

# Reattach the row names
rownames(data_transformed) <- data_row_names
rownames(sh_transformed) <- sh_row_names
```

### Feature selection

**In Pnas RF with labels has been used (!data leakage).**

```{r}
# Ensure reproducibility
set.seed(42)

# Prepare the label and features
label <- data$Type
data_rf_sup <- data %>% select(-c("Type", "AccumulationType", "Individuals"))

# Convert 'Type' to a factor
label <- as.factor(data$Type)

# Train the RandomForest model
rf_model_sup <- randomForest(x = data_rf_sup, y = label, importance = TRUE)

# Get the feature importance
feature_importance_ <- importance(rf_model_sup)

# Print the feature importance
feature_importance_[,c(10:11)][]

```

```{r}
# Selecting MeanDecreaseAccuracy 
mean_decrease_accuracy <- feature_importance_[, "MeanDecreaseAccuracy"]

# Filter features where MeanDecreaseAccuracy is greater than 5
important_features_ <- feature_importance_[mean_decrease_accuracy > 3.4, ]

# Print the important features
important_features_[,c(10:11)]
```

```{r}
# Save top 7 features
columns_rf_sup<- rownames(as.data.frame(important_features_[,c(10:11)]))
columns_rf_sup
```

**Random Forrest feature importance with self-label (without data leakage)**

```{r}
set.seed(42)
data_rf <- data |> select(-c("Type", "AccumulationType", "Individuals"))

# Create artificial labels where each row is its own class
artificial_labels <- seq_len(nrow(data_rf))

# Combine the data with the artificial labels
data_with_labels <- cbind(data_rf, artificial_labels)

# Train Random Forest model
rf_model <- randomForest(artificial_labels ~ ., data = data_with_labels, importance = TRUE)

# Extract feature importance based on Mean Decrease in Gini
feature_importance <- importance(rf_model) # type = 1 for %IncMSE 

feature_importance
```

We are going to use `names(featrue_importance[feature_importance > 2])`

```{r}
# Selecting MeanDecreaseAccuracy 
mean_decrease_accuracy <- feature_importance[,"%IncMSE"]

# Filter features where MeanDecreaseAccuracy is greater than 5
important_features_ <- feature_importance_[mean_decrease_accuracy > 2, ]

# Print the important features
columns_rf <- rownames(important_features_[,c(10:11)])
columns_rf
```

**Unsupervised Feature Selection with Random Forests (UFSRF)**

```{r}
#Todo
```

**SHAP**

```{r}
#Todo

```

**Feature correlation heatmap**

```{r}
data_plot_cor = abbreviated_columns_multi(data)
```

```{r}
# Correlation matrix
cor_matrix <- cor(data_plot_cor[sapply(data_plot_cor, is.numeric)], use = "complete.obs")

melted_cor_matrix <- melt(cor_matrix)
ggplot(melted_cor_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  #theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + # Rotate x-axis labels
  #theme_minimal() +
  labs(title = "Correlation Matrix", x = "", y = "")
```

```{r}
corrplot(cor_matrix, #correlation matrix
         method = "color", #show coloured blocks
         is.corr = TRUE, #input matrix is a correlation matrix
         type = "upper", #show only upper diagonal of correlogram
         order = "AOE", #Hierarchical clustering order
         hclust.method = "average", #using the average linkage distance measure
         addCoef.col = "black", #show numerical value of coefficient
         tl.col = "black", #text color of data label
         tl.cex = 0.5, #text size of data label
         rect.lwd = 3,#text size of data label
         tl.srt = 90, #text rotation of data label
         cl.cex = 1, #text size of color-legend label
         number.cex = 0.5, #text size of correlation coefficient
         diag = FALSE, #hide coefficient on the principal diagonal
)
```

```{r}
corrplot(cor_matrix, #correlation matrix
         method = "color", #show coloured blocks
         is.corr = TRUE, #input matrix is a correlation matrix
         type = "upper", #show only upper diagonal of correlogram
         order = "hclust", #Hierarchical clustering order
         hclust.method = "average", #using the average linkage distance measure
         addCoef.col = "black", #show numerical value of coefficient
         tl.col = "black", #text color of data label
         tl.cex = 0.5, #text size of data label
         rect.lwd = 3,#text size of data label
         tl.srt = 90, #text rotation of data label
         cl.cex = 1, #text size of color-legend label
         number.cex = 0.5, #text size of correlation coefficient
         diag = FALSE, #hide coefficient on the principal diagonal
)
```

**Based on feature correlation and clustering, merge bone subcategories into its categories:**

-   *"Vertebra"= {"Cervical", "Thoracic", "Lumbar", "Sacrum","Rib"}*

-   *"Arm" = {"Humerus", "Radius", "Ulna"}*

-   *"Leg" = {"Femur", "Tibia", "Fibula" }*

-   *"Extremities": {"Carpal", "Tarsal*", "Hand..metacarpals...manual.phalanges.", "Foot..metatarsals...pedal.phal

```{r}
## Define data_cor data set
data_cor = merge_bone_cat_by_mean(data)
```

**Define list of data sets to work with:**

```{r}
list_of_dfs <- list(data, data[columns_rf_sup] , data[columns_rf], data_cor)
```

### Dimension Reduction

**PCA**

```{r}
# PCA
data_pca <- data |> select(-all_of(columns_to_count))
pca_result <- prcomp(data_pca, center = TRUE, scale. = TRUE)
X_pca <- pca_result$x[,1:6]
X_pca
```

```{r}
## Plot 3D pca
df_X_pca = as.data.frame(X_pca)
plot_ly(df_X_pca,x=~PC1,y=~PC2,z=~PC3, color = ~label, type = 'scatter3d', mode = 'markers')


```

```{r}

# Perform PCA
pca_result <- prcomp(data_pca, scale. = TRUE)

# Calculate cumulative variance explained by each component
var_explained <- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))

# Optional: Create a scree plot to visualize the variance explained
ggplot(as.data.frame(var_explained), aes(x = seq_along(var_explained), y = var_explained)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") +
  xlab("Number of Principal Components") +
  ylab("Cumulative Variance Explained") +
  ggtitle("Scree Plot Showing Cumulative Variance Explained by PCA Components")

```

**LDA**

bottom page

```{r, warning=FALSE}
library(MASS)

# LDA
cols_lda <- setdiff(names(data), c("Type","Individuals", "Cluster_Pnas"))
data_lda <- data[cols_lda]
lda_result <- lda(AccumulationType ~ ., data = data_lda)
X_lda <- as.data.frame(predict(lda_result, newdata = data_lda)$x)

# Create a 3D scatter 
plot_ly(X_lda, x = ~LD1, y = ~LD2, z = ~LD3, color = ~data$AccumulationType, type = 'scatter3d', mode = 'markers')
```

```{r}
# LDA
cols_lda <- setdiff(names(data), c("AccumulationType","Individuals", "Cluster_Pnas"))
data_lda <- data[cols_lda]
lda_result <- lda(Type ~ ., data = data_lda)
X_lda <- as.data.frame(predict(lda_result, newdata = data_lda)$x)

# Create a 3D scatter 
plot_ly(X_lda, x = ~LD1, y = ~LD2, z = ~LD3, color = ~data$Type, type = 'scatter3d', mode = 'markers')
```

**amend Dataframe list with X_pca\[,c(1:6)\] and X_lda**

```{R}
list_of_dfs <- list(data_km, data[, columns_rf_sup], data[, columns_rf], data_cor, X_pca[, 1:6], X_lda)
```

### Best K

**Ellbow method**

```{r}
library(cluster)
library(broom)
set.seed(42)
cols_km <- setdiff(names(data), c("Type","AccumulationType","Individuals", "Cluster_Pnas"))
data_km <- data[cols_km]
# Create 10 models with 1 to 10 clusters
kclusts <- tibble(k = 1:7) %>% 
  mutate(
    model = map(k, ~ kmeans(x = data_km, centers = .x, nstart = 20)),
    glanced = map(model, glance)
  ) %>% 
  unnest(cols = c(glanced))

# View results
kclusts%>% 
  ggplot(mapping = aes(x = k, y = tot.withinss)) +
  geom_line(size = 1.2, alpha = 0.5, color = "blue") +
  geom_point(size = 2, color = "blue")
```

```{r}
library(cluster)
library(factoextra)

#create plot of number of clusters vs total within sum of squares
fviz_nbclust(data_km, kmeans, method = "wss")
```

**Silhouette method**

```{r}
fviz_nbclust(data_km, FUN=kmeans, method = "silhouette")
```

```{r}
gap_stat <- clusGap(data_km, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

### Modeling

```{r}
# Load necessary libraries
library(factoextra)
library(cluster)

#Perform K-means clustering
set.seed(42)  
kmeans_result <- kmeans(data_km, centers = 3, nstart = 25)  

# The cluster assignments are stored in kmeans_result$cluster
sub_grp <- kmeans_result$cluster

# Visualize the clusters
fviz_cluster(list(data = data_km, cluster = sub_grp))

```

### Eval

```{r}
# Perform K-means clustering on the training set
set.seed(42) 
kmeans_result <- kmeans(data_km, centers = 3, nstart = 25)

# Assign test set points to nearest clusters
get_nearest_cluster <- function(point, centers) {
  distances <- apply(centers, 1, function(center) dist(rbind(center, point)))
  return(which.min(distances))
}

# Apply the function to each row in the test set
cluster_assignments_test <- apply(sh, 1, get_nearest_cluster, centers = kmeans_result$centers)

sh_km = sh[cols_km] 
# Combine the training and test sets for visualization
combined_df <- rbind(data_km, sh_km)
combined_clusters <- c(kmeans_result$cluster, cluster_assignments_test)

# Visualize with fviz_cluster
fviz_cluster(list(data = combined_df, cluster = combined_clusters))
```

hyperOpt

```{r}
# Load necessary libraries
library(factoextra)
library(cluster)
library(dbscan)


# List of dataframes for evaluation
list_of_dfs <- list(data_km,data[, columns_rf_sup]
                    , data[, columns_rf]
                    #, data_cor 
                    #,X_lda
                    ,X_pca[, 1:6]
                    )

# Number of clusters to evaluate for K-means
k_values <- c(3, 4, 5)

# Hyperparameters for DBSCAN (example ranges)
eps_values <- seq(0.1, 1, 0.2)
minPts_values <- c(5, 10, 15)

# Parameters for hierarchical clustering
hclust_methods <- c("ward.D", "single", "complete", "average")

# List to store results and best models
results <- list()
best_models <- list(kmeans = list(score = -Inf, model = NULL),
                    dbscan = list(score = -Inf, model = NULL),
                    hclust = list(score = -Inf, model = NULL))

# Function to perform K-means clustering
perform_kmeans <- function(data, k, method) {
  set.seed(42)
  kmeans_result <- kmeans(data, centers = k, nstart = 25, algorithm = method)
  silhouette_score <- mean(silhouette(kmeans_result$cluster, dist(data))[, 3])
  return(list(cluster = kmeans_result$cluster, score = silhouette_score))
}

# Function to perform DBSCAN clustering
perform_dbscan <- function(data, eps, minPts) {
  # Perform DBSCAN clustering
  dbscan_result <- dbscan::dbscan(data, eps = eps, minPts = minPts)

  # Exclude noise points (cluster = 0) from silhouette score calculation
  valid_clusters <- dbscan_result$cluster[dbscan_result$cluster != 0]
  valid_data <- data[dbscan_result$cluster != 0, ]

  # Check if there are valid clusters to avoid errors in silhouette calculation
  if (length(unique(valid_clusters)) > 1) {
    silhouette_score <- mean(silhouette(valid_clusters, dist(valid_data))[, 3])
  } else {
    # Assign a default low score if there's only one cluster or none
    silhouette_score <- -1
  }

  return(list(cluster = dbscan_result$cluster, score = silhouette_score))
}


# Function to perform hierarchical clustering
perform_hclust <- function(data, method, k) {
  hc <- hclust(dist(data), method = method)
  clusters <- cutree(hc, k = k)
  silhouette_score <- mean(silhouette(clusters, dist(data))[, 3])
  return(list(cluster = clusters, score = silhouette_score))
}

# Loop through each dataframe
for (i in seq_along(list_of_dfs)) {
  df <- list_of_dfs[[i]]
  
  # K-means
  for (k in k_values) {
    for (method in c("Hartigan-Wong", "Lloyd", "Forgy",
                     "MacQueen")) {
      result <- perform_kmeans(df, k, method)
      if (result$score > best_models$kmeans$score) {
        best_models$kmeans <- list(score = result$score, model = result$cluster, dataframe = i, k = k, method = method)
      }
    }
  }
  
  # DBSCAN
  for (eps in eps_values) {
    for (minPts in minPts_values) {
      result <- perform_dbscan(df, eps, minPts)
      if (result$score > best_models$dbscan$score) {
        best_models$dbscan <- list(score = result$score, model = result$cluster, dataframe = i, eps = eps, minPts = minPts)
      }
    }
  }
  
  # Hierarchical Clustering
  for (method in hclust_methods) {
    for (k in k_values) {
      result <- perform_hclust(df, method, k)
      if (result$score > best_models$hclust$score) {
        best_models$hclust <- list(score = result$score, model = result$cluster, dataframe = i, k = k, method = method)
      }
    }
  }
}

# Best models
best_models
```
