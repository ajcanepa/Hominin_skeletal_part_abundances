---
title: "Hominin skeletal part abundances"
format: html
---

## Dataset

1.  $\frac{MAU}{\max(MNI)}$ (16/36)
2.  $ReMNAU$ (16/36)\
3.  (Maybe) $\frac{MAU} {\max(MNI)}$ for each individual

## MNI vs MNE

-   $\frac{MAU}{\max(MNI)}$ instead of $\frac{MAU}{\max(MAU)}$
-   $MAUI = \frac{MAU}{\max(MNI)}$

## ReMNAU = Relative MNAU

![](images/MNAU%20ReMNAU%20AcReMNAU.png)

# Normalized samples

Normalized samples are independent of max(MAU) or MAX(MNI) $$ \ $$ If $MAUI := \frac{MAU}{\max(MNI)}$, then :

```{=tex}
\begin{split} ReMAUI & =||MAUI ||_1 =ManhattenNorm(\frac{MAU}{max(MNI)}) \\ 

& = \frac{ \frac{MAU}{max(MNI)}}{\sum \frac{MAU}{max(MNI)}} = \frac{MNAU}{\sum MNAU} \\ \\
& =||MNAU||_1 = ReMNAU \end{split} 
```
## AcReMNAU

AcReMNAU = Cumulative % MNAU Used to compare signature of different arq. sites.

![](images/AcReMNAU.jpg)

\## Type (used in PNAS) - Grob vs Fine

-   Primary hominin interment
-   Hominin cannibalism/ secondary interment
-   Nonanthropogenic hominin accumulation
-   Leopard refuse
-   Unscavenged human corpses
-   

## AccumulationType - Old labels

(Total/Used/Future\*) - Burial (10/5/25) - Cannibalism (10/5/14) - Non-human/ non-carnivore intervention (7/3/10) - Carnivore intervention(5/3/10)

\*estimation

## AccumulationType - New labels

(have/future) - Cemeteries (3/7) - Paleolithic Burial(10/25) - Cannibalism (8/9) - Mass grave (2/5+) - Scavenged (3/9+) - Unknown(To be assigned) - Natural Baboon accumulation (1/?) - Carnivore "one bone" (0/5) - Leopard refuse (2/?)

## New Samples

-   Divide Scavenged_NM into 3
-   Add Haglund: Canid Stage 3 & 4
-   waiting for Book for (Un)&Scavenged_WA -\> separate as well
-   Billenhöhle (Cannibalism)
-   Same name leave divided
-   "Purpuré" of Neanderthaler non-burial: Trinkaus 1984

## Niños vs no Niños

-   about 12 with no cut off, difficult to separate

## max(MNI)

-   Still missing 1, 3 uncertain.

## New features

**max(MNI)**:

*Chronology*: - Modern - Upper Paleolithic - Middle Pleistocene Lower Pleistocene - Historic Bronze Age - Neolithic

*Context*: Cave/rockshelter vs Open air vs Volcan

*Animal Bones found*: None Carnivore vs Non-carnivore vs Both

*Spread*: None vs cm vs m vs both (cm & m)

*Kids*: Do we weight kids or adolescents in samples?

## Feature selection

-   All
-   Most important(eg.RF or SHAP)
-   Eliminate most correlated features above (0.85,0.9,0.95)

## Algos

-   KMeans (#cluster, #init, init_algo )
-   KNN (#clusters, weights, algo, metic, p)
-   Hiracical Clustering `hclus()` (method/metric)
-   DBSCAN (maximum distance between, minimum number of points )
-   Gaussian Mixtian Models (#cluster, covariance_type)

## 

## 

## 

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
1 + 1
```

You can add options to executable code like this

```{r}
#| echo: false
2 * 2
```

The `echo: false` option disables the printing of code (only output is displayed).

## Les't go

### first

second place

# Batería de pruebas

-   Data prep with samplewise imputation
    -   Imputation: mean, median, nan(RF), dropNa sample or feature wise, mean.T, median.T, KNN, MICE, Regression, interpolate\
-   EDA
    -   info, head, tail, describe, nunique_etc
    -   hist, distribution plot
    -   correlation matrix
    -   label corr.
    -   Dimension Reduction: PCA, LDA, (t-SNE, UMAP, ANOVA, PHATE, ISOMAP(´step_isomap()´) +)
    -   plots (eg.: Box-Plot, Histogramm, QQ-Diagramm, Streudiagramm, Mosaik-Plot)
-   Outlier dependent on distribution (Isolation Forest, Local Outlier Factor, DBSCAN, One-Class SVM )
-   Feature Selection: drop nan-Feature, drop-variance features, select feature importance RF, drop high-correlation, RFE, (vs MDG, MAD, SHAP)
-   Train/Val/Test split (0.3/0.25/3, Strat = Non, Type, AcType, Cluster_Pnas)
-   Oversampling (Resampling(Boostraping), SMOTE, ADASYN, SMOTE+(BorderlineSmote, KmeanSMOTE) ) + Stratified/NOt Stratified
-   Best K: Ellbow and Silhouette method
-   ML
    -   unsupervised
    -   LDA/pca + unsupervised
    -   supervised
    -   LDA/PCA + supervised
    -   "semi-supervised"
    -   LDA/PCA + "semi-supervised"

## Dataset

Prep - MAU/MNI - ReMAU \## Packages \### Install packages Use RIO

```{r}
install.packages('rio')

```

### Call packages

```{r}
library(rio)
library(tidyverse)
library(ggplot2)
```

## Data loader

### Load the new dataset

```{r}
# Load the new dataset
data <- read_csv('Summary_Dataset_Used_Percentage_MAU.csv')
```

### Preprocess the data using the provided function

Define the `preprocess_function` to: - put "Ref" as index - convert all '-' to NaN's. - convert all columns except the specified to float64 type (!wrong) - divide all numeric columns by 100

```{r}
preprocess_function = function(data) {
  # Set "Ref" as the index (row names in R)
  data <- tibble::column_to_rownames(data, var = "Ref")
  
  # Convert all '-' values to NA in character columns 
  char_cols <- sapply(data, is.character)
  data[char_cols] <- lapply(data[char_cols], na_if, '-')
  
  # Convert all columns except the specified to float64 type (numeric in R)
  cols_to_convert <- setdiff(names(data), c("Type", "AccumulationType", "Cluster_Pnas83"))
  data[cols_to_convert] <- lapply(data[cols_to_convert], function(x) {
    if (is.character(x)) as.numeric(na_if(x, "-")) else x
  })
  
  # Divide all numeric columns by 100
  data[cols_to_convert] <- lapply(data[cols_to_convert], function(x) x / 100)
  
  return(data)
}

```

## EDA

data \|\> select(numeric_columns) \|\> vis_miss()

```{r}
data |> 
  select(numeric_columns) |> 
  vis_miss()
```

### ML packages

pacman / rio

-tidyvers\
-ggplot2 -dplyr -tidyr -readr -purr -tibble -stringr -forcats

tidymodels -tidymodes -rsample -parsnip -recipes -workflows -tune -yardstick -broom -dials

-infer -dicrim -LDA, Naive Byes models

tidycluster

ClusterR

-   library(recipes)
-   library(parsnip)
-   library(tidyr)
-   library(workflows)
-   tidyclust - tune_cluster()
-   library(rsample)
-   library(workflows)
-   library(tune)
-   library(tidyclust)
-   library(tidyverse)
-   library(tidymodels) set.seed(43) \## 20/ etc
-   k_means(num_clusters = tune())

kmeans_spec \<- k_means(num_clusters = tune())

penguins_rec \<- recipe(\~ bill_length_mm + bill_depth_mm, data = penguins )

kmeans_wflow \<- workflow(penguins_rec, kmeans_spec)

clust_num_grid \<- grid_regular(num_clusters(), levels = 10 )

%\>% %\>% clust_num_grid

is workflow and recipe the fit_transform
